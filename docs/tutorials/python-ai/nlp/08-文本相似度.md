---
title: 文本相似度
---

# 文本相似度

> 掌握文本相似度计算方法

## 📚 学习目标

- 掌握编辑距离
- 学会余弦相似度
- 理解语义相似度
- 了解应用场景

## 1. 编辑距离

```python
def levenshtein_distance(s1, s2):
    m, n = len(s1), len(s2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j
    
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if s1[i-1] == s2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j] + 1,    # 删除
                    dp[i][j-1] + 1,    # 插入
                    dp[i-1][j-1] + 1   # 替换
                )
    
    return dp[m][n]

# 使用
distance = levenshtein_distance("kitten", "sitting")
print(f"编辑距离: {distance}")
```

## 2. 余弦相似度

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

texts = [
    "我爱自然语言处理",
    "我喜欢自然语言处理",
    "今天天气很好"
]

# TF-IDF向量化
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(texts)

# 计算相似度
similarity = cosine_similarity(tfidf_matrix)
print("相似度矩阵:\n", similarity)
```

## 3. Jaccard 相似度

```python
def jaccard_similarity(s1, s2):
    set1 = set(s1)
    set2 = set(s2)
    intersection = set1 & set2
    union = set1 | set2
    return len(intersection) / len(union)

# 使用
text1 = list(jieba.cut("我爱自然语言处理"))
text2 = list(jieba.cut("我喜欢自然语言处理"))
sim = jaccard_similarity(text1, text2)
print(f"Jaccard相似度: {sim}")
```

## 4. 语义相似度

```python
# 使用Word2Vec
from gensim.models import Word2Vec

# 训练词向量
sentences = [["我", "爱", "编程"], ["我", "喜欢", "编程"]]
model = Word2Vec(sentences, vector_size=100, min_count=1)

# 计算相似度
similarity = model.wv.similarity('爱', '喜欢')
print(f"'爱'和'喜欢'的相似度: {similarity}")

# 句子相似度
import numpy as np

def sentence_similarity(s1, s2, model):
    words1 = [w for w in jieba.cut(s1) if w in model.wv]
    words2 = [w for w in jieba.cut(s2) if w in model.wv]
    
    if not words1 or not words2:
        return 0
    
    vec1 = np.mean([model.wv[w] for w in words1], axis=0)
    vec2 = np.mean([model.wv[w] for w in words2], axis=0)
    
    return cosine_similarity([vec1], [vec2])[0][0]
```

## 5. 应用：文本查重

```python
def find_similar_documents(query, documents, threshold=0.7):
    # 向量化
    all_texts = [query] + documents
    vectorizer = TfidfVectorizer()
    tfidf = vectorizer.fit_transform(all_texts)
    
    # 计算相似度
    similarities = cosine_similarity(tfidf[0:1], tfidf[1:]).flatten()
    
    # 筛选相似文档
    similar_docs = []
    for idx, sim in enumerate(similarities):
        if sim >= threshold:
            similar_docs.append((idx, documents[idx], sim))
    
    return sorted(similar_docs, key=lambda x: x[2], reverse=True)

# 使用
query = "自然语言处理技术"
docs = [
    "NLP技术发展很快",
    "深度学习在NLP中应用广泛",
    "今天天气很好"
]
results = find_similar_documents(query, docs, threshold=0.3)
for idx, doc, sim in results:
    print(f"文档{idx}: {doc} (相似度: {sim:.4f})")
```

---

**下一节：** [RNN和LSTM](09-RNN和LSTM.md)
