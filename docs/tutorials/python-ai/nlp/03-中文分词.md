---
title: 中文分词
---

# 中文分词

> 掌握中文分词技术和应用

## 📚 学习目标

- 理解中文分词原理
- 掌握jieba分词
- 学会词性标注
- 掌握关键词提取

## 1. jieba 分词

```python
import jieba

text = "我来到北京清华大学"

# 精确模式
seg_list = jieba.cut(text, cut_all=False)
print("精确模式:", "/ ".join(seg_list))

# 全模式
seg_list = jieba.cut(text, cut_all=True)
print("全模式:", "/ ".join(seg_list))

# 搜索引擎模式
seg_list = jieba.cut_for_search(text)
print("搜索模式:", "/ ".join(seg_list))
```

## 2. 词性标注

```python
import jieba.posseg as pseg

text = "我爱自然语言处理技术"
words = pseg.cut(text)

for word, flag in words:
    print(f"{word}\t{flag}")

# 常见词性:
# n - 名词, v - 动词, a - 形容词
# r - 代词, d - 副词, p - 介词
```

## 3. 自定义词典

```python
# 添加自定义词
jieba.add_word('自然语言处理')
jieba.add_word('深度学习', freq=20000, tag='n')

# 删除词
jieba.del_word('自然')

# 加载词典
jieba.load_userdict('userdict.txt')

# userdict.txt 格式:
# 词语 词频 词性
# 自然语言处理 100 n
# 深度学习 200 n
```

## 4. 关键词提取

```python
import jieba.analyse

text = """
自然语言处理是计算机科学的一个重要分支。
它研究人与计算机之间用自然语言进行通信。
深度学习技术在NLP中得到广泛应用。
"""

# TF-IDF
keywords = jieba.analyse.extract_tags(text, topK=5, withWeight=True)
for word, weight in keywords:
    print(f"{word}: {weight:.4f}")

# TextRank
keywords = jieba.analyse.textrank(text, topK=5, withWeight=True)
for word, weight in keywords:
    print(f"{word}: {weight:.4f}")
```

## 5. 其他分词工具

```python
# pkuseg - 北大分词
import pkuseg
seg = pkuseg.pkuseg()
text = "我爱自然语言处理"
result = seg.cut(text)
print(result)

# thulac - 清华分词
import thulac
thu = thulac.thulac()
result = thu.cut(text, text=True)
print(result)
```

---

**下一节：** [词向量](04-词向量.md)
