---
title: 文本生成
---

# 文本生成

> 掌握文本生成技术和应用

## 📚 学习目标

- 理解语言模型
- 掌握文本生成方法
- 学会文本摘要
- 了解对话系统

## 1. 语言模型

```python
import torch
import torch.nn as nn

class LanguageModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        output, hidden = self.lstm(embedded, hidden)
        logits = self.fc(output)
        return logits, hidden

# 生成文本
def generate_text(model, start_text, length=50):
    model.eval()
    with torch.no_grad():
        # 初始化
        input_ids = tokenize(start_text)
        generated = input_ids.copy()
        
        for _ in range(length):
            x = torch.tensor([generated]).long()
            output, _ = model(x)
            next_token = output[0, -1].argmax().item()
            generated.append(next_token)
        
        return decode(generated)
```

## 2. 使用 GPT 生成

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

def generate(prompt, max_length=100):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_beams=5,              # beam search
        no_repeat_ngram_size=2,   # 避免重复
        early_stopping=True,
        temperature=0.8           # 控制随机性
    )
    
    text = tokenizer.decode(output[0], skip_special_tokens=True)
    return text

result = generate("The future of AI is")
print(result)
```

## 3. 文本摘要

```python
from transformers import pipeline

# 使用预训练模型
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

text = """
Long article text here...
"""

summary = summarizer(text, max_length=130, min_length=30, do_sample=False)
print(summary[0]['summary_text'])
```

## 4. 序列到序列

```python
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, src, trg):
        # 编码
        encoder_outputs, hidden = self.encoder(src)
        
        # 解码
        outputs, hidden = self.decoder(trg, hidden)
        
        return outputs

# 用于机器翻译
# src: "I love you"
# trg: "我爱你"
```

## 5. 对话生成

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载对话模型
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")

# 多轮对话
chat_history_ids = None

while True:
    user_input = input("You: ")
    if user_input.lower() == 'quit':
        break
    
    # 编码用户输入
    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token,
                                     return_tensors='pt')
    
    # 拼接历史
    if chat_history_ids is not None:
        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)
    else:
        bot_input_ids = new_input_ids
    
    # 生成回复
    chat_history_ids = model.generate(
        bot_input_ids,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # 解码
    response = tokenizer.decode(
        chat_history_ids[:, bot_input_ids.shape[-1]:][0],
        skip_special_tokens=True
    )
    print(f"Bot: {response}")
```

---

**下一节：** [实战案例](12-实战案例.md)
